from unsloth import FastLanguageModel
import torch
from evaluate import load
import nltk
from bert_score import score as bert_score
from datasets import load_dataset

nltk.download('wordnet')

def load_model(model_name="NV9523/DentalGPT", max_seq_len=1024):
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_name,
        max_seq_length=max_seq_len,
        dtype=None,
        load_in_4bit=True
    )
    FastLanguageModel.for_inference(model)
    return model, tokenizer

def format_prompt(i, q, g, r, j, a, f, c, s):
    base_prompt = (
        "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>"
        "<ÔΩúsystemÔΩú>\n"
        f"### H∆∞·ªõng d·∫´n: {i.strip()}\n"
        "<ÔΩúuserÔΩú>\n"
        f"### C√¢u h·ªèi:\n {q.strip()}\n"
    )
    reference = (
        base_prompt +
        "<ÔΩúthinkÔΩú>\n"
        "H√£y c√πng di·ªÖn gi·∫£i t·ª´ng b∆∞·ªõc n√†o!ü§î\n"
        "<reasoning_cot>\n"
        "# üß† Suy lu·∫≠n c·ªßa DentalGPT\n"
        f"## 1Ô∏è‚É£ M·ª•c ti√™u üìå\n{g.strip()}\n"
        f"## 2Ô∏è‚É£ B∆∞·ªõc suy nghƒ© ‚öôÔ∏è\n{r.strip()}\n"
        f"## 3Ô∏è‚É£ Gi·∫£i th√≠ch üìù\n{j.strip()}\n"
        "</reasoning_cot>\n"
        "<ÔΩúexpertÔΩú>\n"
        "<experting>\n"
        "# üë®‚Äçüî¨ Chuy√™n gia\n"
        f"## Tr√¨nh b√†y d·∫°ng: {f.strip()}\n"
        f"## N·ªôi dung v·ªÅ: {c.strip()}\n"
        f"## Chuy√™n s√¢u v·ªÅ: {s.strip()}\n"
        "</experting>\n"
        "<ÔΩúassistantÔΩú>\n"
        "<answer>\n"
        f"# üí¨ C√¢u tr·∫£ l·ªùi\n{a.strip()}\n"
        "</answer>"
        "<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>"
    )
    return base_prompt, reference

def calculate_perplexity(model, tokenizer, texts):
    encodings = tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to("cuda")
    input_ids = encodings.input_ids
    with torch.no_grad():
        outputs = model(input_ids, labels=input_ids)
        loss = outputs.loss
    return torch.exp(loss).item()

def load_evaluate_dataset(model, tokenizer, dataset_name="NV9523/DentalGPT_SFT"):
    dataset = load_dataset(dataset_name, split="test")

    predictions = []
    references = []

    for ex in dataset:
        prompt, ref = format_prompt(
            ex["Instruction"], ex["C√¢u h·ªèi"],
            ex["CoT_Goal"], ex["CoT_Reasoning"],
            ex["CoT_Justification"], ex["C√¢u tr·∫£ l·ªùi"],
            ex["label1"], ex["label2"], ex["label3"]
        )
        input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
        with torch.no_grad():
            output_ids = model.generate(**input_ids, max_length=1024)
        output = tokenizer.decode(output_ids[0], skip_special_tokens=True)

        predictions.append(output.strip())
        references.append(ref.strip())

    print("=== Sample Output ===")
    for i in range(min(3, len(predictions))):  # Hi·ªÉn th·ªã 3 m·∫´u ƒë·∫ßu
        print(f"\n--- Q{i+1} ---")
        print(f"Prediction:\n{predictions[i]}")
        print(f"Reference:\n{references[i]}")

    # Metrics
    bleu = load("bleu")
    rouge = load("rouge")
    meteor = load("meteor")

    bleu_result = bleu.compute(predictions=predictions, references=references)
    rouge_result = rouge.compute(predictions=predictions, references=references)
    meteor_result = meteor.compute(predictions=predictions, references=references)
    P, R, F1 = bert_score(predictions, references, lang="vi", verbose=False)
    ppl = calculate_perplexity(model, tokenizer, references)

    print("\n=== Evaluation Metrics ===")
    print(f"Perplexity: {ppl:.4f}")
    print(f"BLEU: {bleu_result['bleu']:.4f}")
    print(f"ROUGE: {rouge_result}")
    print(f"METEOR: {meteor_result['meteor']:.4f}")
    print(f"BERTScore F1: {F1.mean().item():.4f}")
