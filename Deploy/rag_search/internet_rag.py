from keybert import KeyBERT
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from vectordb.build_vectordb import chunk_documents
def extract_keywords_keybert(text, top_n=3, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
    kw_model = KeyBERT(model=model_name)
    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=top_n)
    return [kw for kw, _ in keywords]

def Live_Retrieval_Augmented(
    question: str,
    documents: list,
    top_n_keywords: int = 3,
    top_k_docs: int = 3,
    temperature: float = 0.3,
    embedding_model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    keyword_model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
    llm_model_name: str = "models/gemini-1.5-flash-latest",
    prompt_template_str: str = None,
    api_key: str = ""
):
    # 1. Tá»« khÃ³a
    keywords = extract_keywords_keybert(question, top_n=top_n_keywords, model_name=keyword_model_name)
    if not keywords:
        return {"error": "KhÃ´ng tÃ¬m tháº¥y tá»« khÃ³a."}
    keyword_query = " ".join(keywords)

    # 2. Embedding & FAISS
    embedding = HuggingFaceEmbeddings(
        model_name=embedding_model_name,
        model_kwargs={"device": "cuda"}
    )
    chunked_docs = chunk_documents(documents)
    chunked_texts = [doc.page_content for doc in chunked_docs]
    db = FAISS.from_texts(chunked_texts, embedding)
    retriever = db.as_retriever(search_kwargs={"k": top_k_docs})

    # 3. Prompt template
    default_prompt = f"""
    Báº¡n lÃ  má»™t bÃ¡c sÄ© nha khoa chuyÃªn mÃ´n sÃ¢u.
    Dá»±a vÃ o cÃ¡c Ä‘oáº¡n tÃ i liá»‡u sau, hÃ£y táº¡o ngá»¯ cáº£nh cho chatbot nha khoa dá»±a vÃ o cÃ¢u há»i vÃ  tá»« khÃ³a.

    TÃ€I LIá»†U:
    {retriever}

    CÃ‚U Há»I:
    {keyword_query}
    """
    prompt_str = prompt_template_str if prompt_template_str else default_prompt
    prompt = PromptTemplate.from_template(prompt_str)

    # 4. LLM
    llm = ChatGoogleGenerativeAI(
        model=llm_model_name,
        temperature=temperature,
        google_api_key=api_key
    )

    # 5. QA Chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )

    # 6. Káº¿t quáº£
    result = qa_chain({"query": keyword_query})

    return {
        "question": question,
        "keywords": keywords,
        "answer": result["result"],
        "source_documents": [doc.page_content for doc in result["source_documents"]]
    }
# if __name__ == "__main__":
#     documents = [
#         "ViÃªm nha chu á»Ÿ tráº» em lÃ  tÃ¬nh tráº¡ng viÃªm nhiá»…m á»Ÿ mÃ´ quanh rÄƒng, cÃ³ thá»ƒ gÃ¢y cháº£y mÃ¡u, sÆ°ng nÆ°á»›u vÃ  hÃ´i miá»‡ng.",
#         "CÃ¡c triá»‡u chá»©ng bao gá»“m: cháº£y mÃ¡u chÃ¢n rÄƒng khi Ä‘Ã¡nh rÄƒng, nÆ°á»›u sÆ°ng Ä‘á», Ä‘au khi Äƒn uá»‘ng, vÃ  cÃ³ máº£ng bÃ¡m quanh rÄƒng.",
#         "ViÃªm nha chu náº¿u khÃ´ng Ä‘iá»u trá»‹ cÃ³ thá»ƒ gÃ¢y máº¥t rÄƒng sá»›m vÃ  áº£nh hÆ°á»Ÿng Ä‘áº¿n sá»± phÃ¡t triá»ƒn xÆ°Æ¡ng hÃ m cá»§a tráº»."
#     ]

#     result = Live_Retrieval_Augmented(
#         question="Triá»‡u chá»©ng lÃ¢m sÃ ng cá»§a bá»‡nh viÃªm nha chu á»Ÿ tráº» em?",
#         documents=documents,
#         top_n_keywords=3,
#         top_k_docs=3,
#         temperature=0.3,
#         api_key="API_KEY"
#     )

#     print("ğŸ”‘ Keywords:", result["keywords"])
#     print("ğŸ“Œ Question:", result["question"])
#     print("ğŸ¤– Answer:", result["answer"])
#     print("ğŸ“š Sources:")
#     for i, doc in enumerate(result["source_documents"], 1):
#         print(f"[{i}] {doc}")
