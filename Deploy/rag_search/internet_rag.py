from keybert import KeyBERT
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from gemini_tool.call_gemini import get_all_api_keys as gemini_key
from vectordb.build_vectordb import chunk_documents
for k in gemini_key():
    key = k
def extract_keywords_keybert(text, top_n=3, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
    kw_model = KeyBERT(model=model_name)
    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=top_n)
    return [kw for kw, _ in keywords]

def Live_Retrieval_Augmented(
    question: str,
    documents: list,
    top_n_keywords: int = 3,
    top_k_docs: int = 3,
    temperature: float = 0.3,
    embedding_model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    keyword_model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
    llm_model_name: str = "models/gemini-1.5-flash-latest",
    prompt_template_str: str = None,
    api_key: str = k
):
    # 1. T·ª´ kh√≥a
    keywords = extract_keywords_keybert(question, top_n=top_n_keywords, model_name=keyword_model_name)
    if not keywords:
        return {"error": "Kh√¥ng t√¨m th·∫•y t·ª´ kh√≥a."}
    keyword_query = " ".join(keywords)

    # 2. Embedding & FAISS
    embedding = HuggingFaceEmbeddings(
        model_name=embedding_model_name,
        model_kwargs={"device": "cuda"}
    )
    chunked_docs = chunk_documents(documents)
    chunked_texts = [doc.page_content for doc in chunked_docs]
    db = FAISS.from_texts(chunked_texts, embedding)
    retriever = db.as_retriever(search_kwargs={"k": top_k_docs})

    # 3. Prompt template
    default_prompt = """
    B·∫°n l√† m·ªôt b√°c sƒ© nha khoa chuy√™n m√¥n s√¢u.
    D·ª±a v√†o c√°c ƒëo·∫°n t√†i li·ªáu sau, h√£y t·∫°o ng·ªØ c·∫£nh cho chatbot nha khoa d·ª±a v√†o c√¢u h·ªèi v√† t·ª´ kh√≥a.

    T√ÄI LI·ªÜU:
    {context}

    C√ÇU H·ªéI:
    {question}
    """
    prompt_str = prompt_template_str if prompt_template_str else default_prompt
    prompt = PromptTemplate.from_template(prompt_str)

    # 4. LLM
    llm = ChatGoogleGenerativeAI(
        model=llm_model_name,
        temperature=temperature,
        google_api_key=api_key
    )

    # 5. QA Chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )

    # 6. K·∫øt qu·∫£
    result = qa_chain({"query": keyword_query})

    return {
        "question": question,
        "keywords": keywords,
        "answer": result["result"],
        "source_documents": [doc.page_content for doc in result["source_documents"]]
    }
# if __name__ == "__main__":
#     documents = [
#         "Vi√™m nha chu ·ªü tr·∫ª em l√† t√¨nh tr·∫°ng vi√™m nhi·ªÖm ·ªü m√¥ quanh rƒÉng, c√≥ th·ªÉ g√¢y ch·∫£y m√°u, s∆∞ng n∆∞·ªõu v√† h√¥i mi·ªáng.",
#         "C√°c tri·ªáu ch·ª©ng bao g·ªìm: ch·∫£y m√°u ch√¢n rƒÉng khi ƒë√°nh rƒÉng, n∆∞·ªõu s∆∞ng ƒë·ªè, ƒëau khi ƒÉn u·ªëng, v√† c√≥ m·∫£ng b√°m quanh rƒÉng.",
#         "Vi√™m nha chu n·∫øu kh√¥ng ƒëi·ªÅu tr·ªã c√≥ th·ªÉ g√¢y m·∫•t rƒÉng s·ªõm v√† ·∫£nh h∆∞·ªüng ƒë·∫øn s·ª± ph√°t tri·ªÉn x∆∞∆°ng h√†m c·ªßa tr·∫ª."
#     ]

#     result = Live_Retrieval_Augmented(
#         question="Tri·ªáu ch·ª©ng l√¢m s√†ng c·ªßa b·ªánh vi√™m nha chu ·ªü tr·∫ª em?",
#         documents=documents,
#         top_n_keywords=3,
#         top_k_docs=3,
#         temperature=0.3,
#         api_key="API_KEY"
#     )

#     print("üîë Keywords:", result["keywords"])
#     print("üìå Question:", result["question"])
#     print("ü§ñ Answer:", result["answer"])
#     print("üìö Sources:")
#     for i, doc in enumerate(result["source_documents"], 1):
#         print(f"[{i}] {doc}")
