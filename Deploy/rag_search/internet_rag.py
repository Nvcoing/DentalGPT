from keybert import KeyBERT
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from vectordb.build_vectordb import chunk_documents

def extract_keywords_keybert(text, top_n=3, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
    kw_model = KeyBERT(model=model_name)
    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=top_n)
    return [kw for kw, _ in keywords]

def Live_Retrieval_Augmented(
    question: str,
    documents: list,
    top_n_keywords: int = 3,
    top_k_docs: int = 3,
    embedding_model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    keyword_model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"
):
    # 1. TrÃ­ch xuáº¥t tá»« khÃ³a tá»« cÃ¢u há»i
    keywords = extract_keywords_keybert(question, top_n=top_n_keywords, model_name=keyword_model_name)
    if not keywords:
        return {"error": "KhÃ´ng tÃ¬m tháº¥y tá»« khÃ³a."}
    keyword_query = " ".join(keywords)

    # 2. Chunk vÃ  táº¡o FAISS index
    embedding = HuggingFaceEmbeddings(
        model_name=embedding_model_name,
        model_kwargs={"device": "cuda"}
    )
    chunked_docs = chunk_documents(documents)
    chunked_texts = [doc.page_content for doc in chunked_docs]
    db = FAISS.from_texts(chunked_texts, embedding)

    # 3. Truy xuáº¥t cÃ¡c Ä‘oáº¡n vÄƒn báº£n phÃ¹ há»£p vá»›i keyword query
    retriever = db.as_retriever(search_kwargs={"k": top_k_docs})
    retrieved_docs = retriever.get_relevant_documents(keyword_query)

    return {
        "question": question,
        "keywords": keywords,
        "document": [doc.page_content for doc in retrieved_docs]
    }

# if __name__ == "__main__":
#     documents = [
#         "ViÃªm nha chu á»Ÿ tráº» em lÃ  tÃ¬nh tráº¡ng viÃªm nhiá»…m á»Ÿ mÃ´ quanh rÄƒng, cÃ³ thá»ƒ gÃ¢y cháº£y mÃ¡u, sÆ°ng nÆ°á»›u vÃ  hÃ´i miá»‡ng.",
#         "CÃ¡c triá»‡u chá»©ng bao gá»“m: cháº£y mÃ¡u chÃ¢n rÄƒng khi Ä‘Ã¡nh rÄƒng, nÆ°á»›u sÆ°ng Ä‘á», Ä‘au khi Äƒn uá»‘ng, vÃ  cÃ³ máº£ng bÃ¡m quanh rÄƒng.",
#         "ViÃªm nha chu náº¿u khÃ´ng Ä‘iá»u trá»‹ cÃ³ thá»ƒ gÃ¢y máº¥t rÄƒng sá»›m vÃ  áº£nh hÆ°á»Ÿng Ä‘áº¿n sá»± phÃ¡t triá»ƒn xÆ°Æ¡ng hÃ m cá»§a tráº»."
#     ]

#     result = Live_Retrieval_Augmented(
#         question="Triá»‡u chá»©ng lÃ¢m sÃ ng cá»§a bá»‡nh viÃªm nha chu á»Ÿ tráº» em?",
#         documents=documents,
#         top_n_keywords=3,
#         top_k_docs=3,
#         temperature=0.3,
#         api_key="API_KEY"
#     )

#     print("ğŸ”‘ Keywords:", result["keywords"])
#     print("ğŸ“Œ Question:", result["question"])
#     print("ğŸ¤– Answer:", result["answer"])
#     print("ğŸ“š Sources:")
#     for i, doc in enumerate(result["source_documents"], 1):
#         print(f"[{i}] {doc}")
