# import os
# import requests
# import logging
# from bs4 import BeautifulSoup
# from googleapiclient.discovery import build
# from dotenv import load_dotenv

# # Load bi·∫øn m√¥i tr∆∞·ªùng
# dotenv_path = os.path.join(os.path.dirname(__file__), '.env')
# load_dotenv(dotenv_path=dotenv_path)
# API_KEY = os.getenv("GOOGLE_API_KEY")
# CSE_ID = os.getenv("GOOGLE_CSE_ID")

# # Logging setup
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# # C√°c ƒë·ªãnh d·∫°ng kh√¥ng c·∫ßn tr√≠ch HTML
# NON_HTML_TYPES = {
#     'pdf', 'doc', 'docx', 'ppt', 'pptx', 'xls', 'xlsx', 'csv', 'txt', 'rtf',
#     'jpg', 'jpeg', 'png', 'gif', 'bmp', 'svg',
#     'mp4', 'avi', 'mov', 'wmv', 'flv', 'mkv', 'webm'
# }

# def get_page_content(url):
#     try:
#         response = requests.get(url, timeout=10)
#         response.raise_for_status()
#         soup = BeautifulSoup(response.text, 'html.parser')
#         paragraphs = soup.find_all('p')
#         content = "\n".join([p.get_text() for p in paragraphs])
#         return content.strip()
#     except requests.exceptions.RequestException as e:
#         logger.warning(f"[L·ªñI] Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung t·ª´ {url}: {e}")
#         return ""


# # H√†m t√¨m ki·∫øm g·ªëc
# def tool_search(query, file_type=None, num_results=3, api_key=API_KEY, cse_id=CSE_ID):
#     if not api_key and not cse_id:
#         logger.error("API_KEY ho·∫∑c CSE_ID kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y.")
#         return []

#     try:
#         search_query = query
#         if file_type:
#             search_query += f" filetype:{file_type}"

#         service = build("customsearch", "v1", developerKey=api_key)
#         res = service.cse().list(
#             q=search_query,
#             cx=cse_id,
#             num=num_results,
#             fileType=file_type if file_type else None
#         ).execute()

#         results = []
#         if 'items' in res:
#             for item in res['items']:
#                 link = item.get('link')
#                 title = item.get('title')
#                 snippet = item.get('snippet')

#                 if file_type and file_type.lower() in NON_HTML_TYPES:
#                     content = f"[T·ªáp {file_type.upper()}] Kh√¥ng tr√≠ch xu·∫•t n·ªôi dung."
#                 else:
#                     content = get_page_content(link)

#                 results.append({
#                     'title': title,
#                     'link': link,
#                     'snippet': snippet,
#                     'content': content
#                 })
#         else:
#             logger.info("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£.")
#         return results
#     except Exception as e:
#         logger.error(f"L·ªói trong tool_search: {e}")
#         return []

# # V√≠ d·ª• test
# if __name__ == "__main__":
#     query1 = "T√¨m t√†i li·ªáu tr√≠ tu·ªá nh√¢n t·∫°o"
#     query2 = "Cho t√¥i t√†i li·ªáu"
#     print("üîç T√¨m ki·∫øm th∆∞·ªùng:")
#     results = tool_search(query1)
#     for i, r in enumerate(results):
#         print(f"{i+1}. {r['title']}\n   {r['link']}\n   {r['content'][:300]}\n")

#     print("üìÑ T√¨m ki·∫øm c√≥ ƒë·ªãnh d·∫°ng:")
#     results2 = tool_search(query2)
#     for i, r in enumerate(results2):
#         print(f"{i+1}. {r['title']}\n   {r['link']}\n   {r['content'][:300]}\n")
import os
import requests
import logging
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
from dotenv import load_dotenv

# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
dotenv_path = os.path.join(os.path.dirname(__file__), '.env')
load_dotenv(dotenv_path=dotenv_path)

API_KEY = os.getenv("GOOGLE_API_KEY")
CSE_ID = os.getenv("GOOGLE_CSE_ID")

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# H√†m l·∫•y n·ªôi dung trang web
def get_page_content(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        paragraphs = soup.find_all('p')
        content = "\n".join([p.get_text() for p in paragraphs])
        return content.strip()
    except requests.exceptions.RequestException as e:
        logger.warning(f"[L·ªñI] Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung t·ª´ {url}: {e}")
        return ""

# H√†m ch√≠nh: th·ª±c hi·ªán t√¨m ki·∫øm v√† tr√≠ch xu·∫•t n·ªôi dung
def tool_search(query, api_key=API_KEY, cse_id=CSE_ID, num_results=10):
    if not api_key or not cse_id:
        logger.error("API_KEY ho·∫∑c CSE_ID kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong bi·∫øn m√¥i tr∆∞·ªùng.")
        return []

    try:
        service = build("customsearch", "v1", developerKey=api_key)
        res = service.cse().list(q=query, cx=cse_id, num=num_results).execute()

        results = []
        if 'items' in res:
            for item in res['items']:
                link = item.get('link')
                content = get_page_content(link)
                results.append({
                    'title': item.get('title'),
                    'link': link,
                    'snippet': item.get('snippet'),
                    'content': content
                })
        else:
            logger.info("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p.")
        return results
    except Exception as e:
        logger.error(f"L·ªói trong tool_search: {e}")
        return []

# # V√≠ d·ª• g·ªçi h√†m
# if __name__ == "__main__":
#     query = "DFT technology JSC l√† c√¥ng ty g√¨"
#     search_results = tool_search(query)

#     for i, item in enumerate(search_results):
#         print(f"{i+1}. üè∑Ô∏è {item['title']}")
#         print(f"   üîó {item['link']}")
#         print(f"   ‚úçÔ∏è  {item['snippet']}")
#         print(f"   üìÑ N·ªôi dung:\n{item['content'][:800]}")  # In t·ªëi ƒëa 800 k√Ω t·ª±
#         print("-" * 80)
