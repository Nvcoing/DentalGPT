import requests
import time
from config import NGROK_URL
from gemini_tool.call_gemini import call_gemini

# === Prompt Ä‘áº§u vÃ o cho LLM Server ná»™i bá»™ ===
def build_prompt(prompt: str) -> str:
    return (
        "<ï½œbeginâ–ofâ–sentenceï½œ>"
        "<ï½œsystemï½œ>\n"
        "### HÆ°á»›ng dáº«n: HÃ£y lÃ  má»™t trá»£ lÃ½ áº£o nha khoa vÃ  TRÃŒNH BÃ€Y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i dÆ°á»›i Ä‘Ã¢y:\n"
        "<ï½œuserï½œ>\n"
        f"### CÃ¢u há»i:\n{prompt.strip()}\n"
    )

# === Format Ä‘áº§u ra chuáº©n yÃªu cáº§u ===
def format_final_output(answer: str) -> str:
    return (
        "<ï½œassistantï½œ>\n"
        "<answer>\n"
        f"# ğŸ’¬ CÃ¢u tráº£ lá»i\n{answer.strip()}\n"
        "</answer>"
        "<ï½œendâ–ofâ–sentenceï½œ>"
    )

# === Gá»­i yÃªu cáº§u Ä‘áº¿n LLM server custom ===
def send_request(prompt: str, generation_params: dict):
    try:
        response = requests.post(NGROK_URL, json={"prompt": prompt, **generation_params}, stream=True)
        response.raise_for_status()
        return response
    except requests.exceptions.RequestException as e:
        return f"Error during generation: {str(e)}"

# === Táº¡o cÃ¡c prompt phá»¥ cho Gemini Ä‘á»ƒ pháº£n biá»‡n vÃ  cáº£i tiáº¿n ===
def build_gemini_prompts(question_prompt: str, raw_answer: str) -> dict:
    return {
        "study_method": f"""Báº¡n lÃ  chuyÃªn gia nha khoa. Vá»›i cÃ¢u há»i: "{question_prompt}", cÃ¢u tráº£ lá»i hiá»‡n táº¡i: "{raw_answer}".
HÃ£y liá»‡t kÃª chi tiáº¿t táº¥t cáº£ cÃ¡c **phÆ°Æ¡ng phÃ¡p Ä‘iá»u trá»‹, ká»¹ thuáº­t vÃ  quy trÃ¬nh** cÃ³ thá»ƒ Ã¡p dá»¥ng. Bao gá»“m mÃ´ táº£, khi nÃ o Ã¡p dá»¥ng, báº£ng markdown náº¿u cáº§n.""",

        "fix_presentation": f"""CÃ¢u tráº£ lá»i: "{raw_answer}". Cáº£i thiá»‡n cÃ¡ch trÃ¬nh bÃ y cho rÃµ rÃ ng, chuyÃªn nghiá»‡p hÆ¡n. CÃ³ thá»ƒ dÃ¹ng báº£ng markdown, danh sÃ¡ch, cÃ´ng thá»©c chuáº©n dáº¡ng markdown hiá»ƒn thá»‹ hoáº·c biá»ƒu Ä‘á»“ minh há»a báº±ng code python náº¿u phÃ¹ há»£p.""",

        "knowledge_boost": f"""CÃ¢u tráº£ lá»i: "{raw_answer}". Bá»• sung thÃªm kiáº¿n thá»©c giÃºp há»c há»i nhÆ°:
- Thá»‘ng kÃª (cÃ³ thá»ƒ gáº§n Ä‘Ãºng),
- Tá»· lá»‡ thÃ nh cÃ´ng,
- Thá»i gian Ä‘iá»u trá»‹,
- LÆ°u Ã½ lÃ¢m sÃ ng,
- Biáº¿n chá»©ng phá»• biáº¿n.
CÃ³ thá»ƒ tá»± táº¡o sá»‘ liá»‡u gáº§n Ä‘Ãºng nhÆ°ng há»£p lÃ½."""
    }

# === Tá»•ng há»£p cÃ¡c pháº§n láº¡i vÃ  yÃªu cáº§u Gemini tráº£ vá» cÃ¢u tráº£ lá»i hoÃ n chá»‰nh Ä‘Ãºng Ä‘á»‹nh dáº¡ng ===
def build_final_synthesis_prompt(question_prompt: str, improved_parts: dict) -> str:
    return f"""
        Báº¡n lÃ  má»™t trá»£ lÃ½ áº£o nha khoa. HÃ£y tá»•ng há»£p cÃ¡c pháº§n sau Ä‘á»ƒ táº¡o ra má»™t cÃ¢u tráº£ lá»i chuyÃªn sÃ¢u, rÃµ rÃ ng vÃ  chuáº©n nha khoa.

        YÃªu cáº§u:
        - TrÃ¬nh bÃ y chi tiáº¿t, Ä‘áº§y Ä‘á»§.
        - DÃ¹ng báº£ng markdown náº¿u phÃ¹ há»£p.
        - CÃ³ mÃ£ Python váº½ biá»ƒu Ä‘á»“ náº¿u cÃ³ dá»¯ liá»‡u.
        - VÃ  **pháº£i tráº£ Ä‘Ãºng Ä‘á»‹nh dáº¡ng bÃªn dÆ°á»›i** â€” khÃ´ng Ä‘Æ°á»£c thay Ä‘á»•i:
        <ï½œassistantï½œ>
        <answer>
        ğŸ’¬ CÃ¢u tráº£ lá»i
        [Ná»™i dung tráº£ lá»i chi tiáº¿t á»Ÿ Ä‘Ã¢y...]
        </answer>
        <ï½œendâ–ofâ–sentenceï½œ>
        DÆ°á»›i Ä‘Ã¢y lÃ  thÃ´ng tin báº¡n cáº§n tá»•ng há»£p:
        1. PhÆ°Æ¡ng phÃ¡p: {improved_parts['study_method']}
        2. TrÃ¬nh bÃ y láº¡i: {improved_parts['fix_presentation']}
        3. Kiáº¿n thá»©c bá»• sung: {improved_parts['knowledge_boost']}

        CÃ¢u há»i gá»‘c lÃ : {question_prompt}

        â—**LÆ°u Ã½**: Chá»‰ tráº£ Ä‘Ãºng Ä‘á»‹nh dáº¡ng trÃªn. KhÃ´ng thÃªm chá»¯ ngoÃ i Ä‘á»‹nh dáº¡ng.
        """

# === HÃ m chÃ­nh stream káº¿t quáº£ tá»«ng dÃ²ng ===
def generate_response(prompt: str,
                      temperature=0.1, top_p=0.9, top_k=50,
                      repetition_penalty=1.0, do_sample=True,
                      max_new_tokens=1024):
    
    generation_params = {
        "max_new_tokens": max_new_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "top_k": top_k,
        "repetition_penalty": repetition_penalty,
        "do_sample": do_sample
    }

    # B1: Gá»­i cÃ¢u há»i Ä‘áº¿n LLM server láº¥y cÃ¢u tráº£ lá»i Ä‘áº§u tiÃªn
    base_prompt = build_prompt(prompt)
    initial_response = send_request(base_prompt, generation_params)
    

    raw_answer = initial_response.text.strip()

    # B2: Táº¡o 3 prompt phá»¥ vÃ  gá»­i tá»›i Gemini
    sub_prompts = build_gemini_prompts(prompt, raw_answer)
    improved_parts = {}
    for key, sub_prompt in sub_prompts.items():
        try:
            improved_parts[key] = call_gemini(sub_prompt, model_name="models/gemini-2.0-flash")
        except Exception as e:
            improved_parts[key] = f"Lá»—i khi gá»i Gemini ({key}): {str(e)}"

    # B3: Táº¡o prompt tá»•ng há»£p vÃ  gá»­i tá»›i Gemini Ä‘á»ƒ nháº­n cÃ¢u tráº£ lá»i hoÃ n chá»‰nh
    try:
        final_synthesis_prompt = build_final_synthesis_prompt(prompt, improved_parts)
        final_answer = call_gemini(final_synthesis_prompt, model_name="models/gemini-2.0-flash")
        formatted_output = final_answer.strip()
    except Exception as e:
        yield f"Error generating final answer: {str(e)}"
        return

    # B4: Tráº£ káº¿t quáº£ ra theo tá»«ng dÃ²ng
    for line in formatted_output.splitlines(keepends=True):
        yield line