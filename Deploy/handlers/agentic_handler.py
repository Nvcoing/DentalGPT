import requests
import time
from gemini_tool.call_gemini import call_gemini
from config import NGROK_URL

# Template m·∫´u cho 3 lo·∫°i b√°o c√°o
TEMPLATES = {
    "report": [
        "Th√¥ng tin b·ªánh nh√¢n",
        "Tri·ªáu ch·ª©ng l√¢m s√†ng",
        "Ch·∫©n ƒëo√°n s∆° b·ªô",
        "ƒêi·ªÅu tr·ªã v√† l·ªùi khuy√™n"
    ],
    "thesis": [
        "Gi·ªõi thi·ªáu ƒë·ªÅ t√†i",
        "T·ªïng quan t√†i li·ªáu",
        "Ph∆∞∆°ng ph√°p nghi√™n c·ª©u",
        "K·∫øt qu·∫£ v√† th·∫£o lu·∫≠n",
        "K·∫øt lu·∫≠n v√† ki·∫øn ngh·ªã"
    ],
    "paper": [
        "T√≥m t·∫Øt",
        "Gi·ªõi thi·ªáu",
        "V·∫≠t li·ªáu v√† ph∆∞∆°ng ph√°p",
        "K·∫øt qu·∫£",
        "Th·∫£o lu·∫≠n",
        "K·∫øt lu·∫≠n"
    ]
}

# X√¢y d·ª±ng prompt c∆° b·∫£n cho LLM
def build_prompt(question: str) -> str:
    return (
        "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>"
        "<ÔΩúsystemÔΩú>\n"
        "### H∆∞·ªõng d·∫´n: H√£y l√† m·ªôt tr·ª£ l√Ω ·∫£o nha khoa v√† tr√¨nh b√†y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi d∆∞·ªõi ƒë√¢y:\n"
        "<ÔΩúuserÔΩú>\n"
        f"### C√¢u h·ªèi:\n{question.strip()}\n"
    )

# G·ª≠i prompt t·ªõi server LLM
def send_request(prompt: str, generation_params: dict):
    try:
        response = requests.post(NGROK_URL, json={"prompt": prompt, **generation_params}, stream=True)
        response.raise_for_status()
        return response
    except requests.exceptions.RequestException as e:
        return f"Error during generation: {str(e)}"

# X√°c ƒë·ªãnh lo·∫°i b√°o c√°o t·ª´ n·ªôi dung ƒë·∫ßu v√†o
def detect_template_type(prompt: str) -> str:
    prompt_lower = prompt.lower()
    if any(kw in prompt_lower for kw in ["b√°o c√°o", "h·ªì s∆°", "phi·∫øu kh√°m"]):
        return "report"
    elif any(kw in prompt_lower for kw in ["lu·∫≠n vƒÉn", "lu·∫≠n √°n", "thesis", "ƒë·ªÅ t√†i"]):
        return "thesis"
    elif any(kw in prompt_lower for kw in ["paper", "b√†i b√°o khoa h·ªçc"]):
        return "paper"
    return "report"  # M·∫∑c ƒë·ªãnh

# H√†m ch√≠nh g·ªçi t·ª´ FastAPI
def generate_response(prompt: str,
                      temperature=0.1, top_p=0.9, top_k=50,
                      repetition_penalty=1.0, do_sample=True,
                      max_new_tokens=1024):

    generation_params = {
        "max_new_tokens": max_new_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "top_k": top_k,
        "repetition_penalty": repetition_penalty,
        "do_sample": do_sample
    }

    # X√°c ƒë·ªãnh lo·∫°i template v√† c√°c m·ª•c c·∫ßn sinh
    template_type = detect_template_type(prompt)
    sections = TEMPLATES[template_type]
    results = {}

    # Sinh n·ªôi dung t·ª´ng m·ª•c
    for section in sections:
        question = f"{prompt.strip()} - {section}"
        prompt_for_section = build_prompt(question)
        response = send_request(prompt_for_section, generation_params)

        if isinstance(response, str):
            yield response
            return

        output_text = ""
        for chunk in response.iter_content(chunk_size=None):
            if chunk:
                decoded = chunk.decode("utf-8")
                output_text += decoded
                yield decoded  # stream ra ngo√†i cho FastAPI client

        results[section] = output_text.strip()
        time.sleep(0.3)

    # Sau khi xong, t·ªïng h·ª£p l·∫°i v√† g·ª≠i cho Gemini ƒë·ªÉ k·∫øt lu·∫≠n
    summary_prompt = (
        f"""B·∫°n l√† chuy√™n gia nha khoa. D∆∞·ªõi ƒë√¢y l√† c√°c m·ª•c ƒë√£ ƒë∆∞·ª£c ho√†n th√†nh trong {template_type}:\n\n"""
        + "\n\n".join([f"### {k}:\n{v}" for k, v in results.items()])
        + "\n\nH√£y vi·∫øt m·ªôt k·∫øt lu·∫≠n t·ªïng quan, ƒë√°nh gi√° t·ªïng th·ªÉ v√† ƒë·ªÅ xu·∫•t c·∫£i thi·ªán n·∫øu c√≥."
    )

    final_summary = call_gemini(summary_prompt, model_name="models/gemini-1.5-flash-latest")
    yield "\n\nüìå K·∫æT LU·∫¨N T·ªîNG H·ª¢P:\n"
    yield final_summary
    return
