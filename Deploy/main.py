from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import requests
import time
app = FastAPI()

# C·∫•u h√¨nh CORS ƒë·ªÉ cho ph√©p truy c·∫≠p t·ª´ m·ªçi ngu·ªìn
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Cho ph√©p t·∫•t c·∫£ c√°c ngu·ªìn
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# URL c·ªßa m√¥ h√¨nh x·ª≠ l√Ω vƒÉn b·∫£n (c·∫≠p nh·∫≠t n·∫øu c·∫ßn)
NGROK_URL = " https://8d8a-34-125-84-96.ngrok-free.app/model/generate/"

@app.post("/DentalGPT/chatbot/")
async def generate(request: Request):
    req_json = await request.json()
    prompt = req_json.get("prompt")
    mode = req_json.get("mode", "normal")  # M·∫∑c ƒë·ªãnh l√† ch·∫ø ƒë·ªô 'normal'
    max_new_tokens = req_json.get("max_new_tokens", 1024)
    temperature = req_json.get("temperature", 0.7)
    top_p = req_json.get("top_p", 0.9)
    top_k = req_json.get("top_k", 50)
    repetition_penalty = req_json.get("repetition_penalty", 1.0)
    do_sample = req_json.get("do_sample", True)
    num_samples = 1
    if not prompt:
        return JSONResponse(status_code=400, content={"error": "Missing 'prompt' in request"})

    # T·∫°o prompt d·ª±a tr√™n ch·∫ø ƒë·ªô
    if mode == "reason":
        max_new_tokens = 1024  # Gi·ªõi h·∫°n s·ªë token cho ch·∫ø ƒë·ªô 'reason'
        full_prompt = (
            "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>"
            "<ÔΩúsystemÔΩú>\n"
            "### H∆∞·ªõng d·∫´n: H√£y l√† m·ªôt tr·ª£ l√Ω ·∫£o nha khoa v√† SUY LU·∫¨N ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi d∆∞·ªõi ƒë√¢y:\n"
            "<ÔΩúuserÔΩú>\n"
            f"### C√¢u h·ªèi:\n{prompt.strip()}\n"
        )
    elif mode == "deep_reason":
        num_samples = 3
        max_new_tokens = 512
        full_prompt = (
            "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>"
            "<ÔΩúsystemÔΩú>\n"
            "### H∆∞·ªõng d·∫´n: H√£y l√† m·ªôt tr·ª£ l√Ω ·∫£o nha khoa v√† TR√åNH B√ÄY ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi d∆∞·ªõi ƒë√¢y:\n"
            "<ÔΩúuserÔΩú>\n"
            f"### C√¢u h·ªèi:\n{prompt.strip()}\n"
        )
    else:
        max_new_tokens = 512  # Gi·ªõi h·∫°n s·ªë token cho ch·∫ø ƒë·ªô 'normal'
        full_prompt = (
            "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>"
            "<ÔΩúsystemÔΩú>\n"
            "### H∆∞·ªõng d·∫´n: H√£y l√† l√† m·ªôt tr·ª£ l√Ω ·∫£o nha khoa v√† tr·∫£ l·ªùi c√¢u h·ªèi d∆∞·ªõi ƒë√¢y:\n"
            "<ÔΩúuserÔΩú>\n"
            f"### C√¢u h·ªèi:\n{prompt.strip()}\n"
            "<ÔΩúthinkÔΩú>\n"
            "H√£y c√πng di·ªÖn gi·∫£i t·ª´ng b∆∞·ªõc n√†o!ü§î\n"
            "<reasoning_cot>\n"
            "# üß† Suy lu·∫≠n c·ªßa DentalGPT\n"
            f"## 1Ô∏è‚É£ M·ª•c ti√™u üìå\nTr·∫£ l·ªùi ƒë∆°n gi·∫£n, ƒë√∫ng tr·ªçng t√¢m, ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu\n"
            f"## 2Ô∏è‚É£ B∆∞·ªõc suy nghƒ© ‚öôÔ∏è\nB∆∞·ªõc 1: X√°c ƒë·ªãnh ƒë√∫ng c√¢u h·ªèi\nB∆∞·ªõc 2: X√°c ƒë·ªãnh c√¢u tr·∫£ l·ªùi\nB∆∞·ªõc 3: X√°c ƒë·ªãnh c√°ch tr√¨nh b√†y\n"
            f"## 3Ô∏è‚É£ Gi·∫£i th√≠ch üìù\nGi·∫£i th√≠ch ng·∫Øn g·ªçn\n"
            "</reasoning_cot>\n"
        )
    
    data = {
        "prompt": full_prompt,
        "max_new_tokens": max_new_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "top_k": top_k,
        "repetition_penalty": repetition_penalty,
        "do_sample": do_sample
    }
    deep_response = ""
    for _ in range(num_samples):
        deep_response+=full_prompt
        try:
            response = requests.post(NGROK_URL, json=data, stream=True)
            response.raise_for_status()
            deep_response+=f"\n{response}"
            time.sleep(0.5)  # tr√°nh g·ª≠i qu√° nhanh g√¢y l·ªói server
        except requests.exceptions.RequestException as e:
            return JSONResponse(status_code=500, content={"error": f"Error during generation: {str(e)}"})

    def event_stream():
        for chunk in response.iter_content(chunk_size=None):
            if chunk:
                yield chunk.decode("utf-8")

    return StreamingResponse(event_stream(), media_type="text/markdown")
